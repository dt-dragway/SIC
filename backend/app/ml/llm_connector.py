"""
SIC Ultra - Conector LLM para AnÃ¡lisis Inteligente

Conecta con modelos de lenguaje profesionales:
- DeepSeek API
- OpenAI GPT-4
- Ollama (local, sin costo)

El LLM analiza el contexto del mercado y proporciona
razonamiento avanzado para las seÃ±ales de trading.
"""

import httpx
from typing import Optional, Dict, List
from datetime import datetime
from loguru import logger
from abc import ABC, abstractmethod

from app.config import settings


# === Base LLM Interface ===

class LLMProvider(ABC):
    """Interface base para proveedores de LLM"""
    
    @abstractmethod
    async def analyze(self, prompt: str) -> Optional[str]:
        """Enviar prompt y obtener respuesta"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """Verificar si el proveedor estÃ¡ disponible"""
        pass


# === DeepSeek Provider ===

class DeepSeekProvider(LLMProvider):
    """
    Conector para DeepSeek API.
    
    DeepSeek es mÃ¡s econÃ³mico que OpenAI y tiene buen rendimiento
    para anÃ¡lisis financiero.
    
    Pricing: ~$0.14 / 1M tokens (muy econÃ³mico)
    """
    
    BASE_URL = "https://api.deepseek.com/v1/chat/completions"
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or getattr(settings, 'deepseek_api_key', None)
        self.model = "deepseek-chat"  # Modelo principal
    
    def is_available(self) -> bool:
        return bool(self.api_key)
    
    async def analyze(self, prompt: str) -> Optional[str]:
        if not self.is_available():
            return None
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.BASE_URL,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {
                                "role": "system",
                                "content": """Eres un analista de trading profesional. 
                                Analizas datos de mercado y proporcionas seÃ±ales claras.
                                Responde siempre en espaÃ±ol y sÃ© conciso.
                                Formato: SIGNAL: BUY/SELL/HOLD, CONFIDENCE: %, REASON: ..."""
                            },
                            {
                                "role": "user",
                                "content": prompt
                            }
                        ],
                        "temperature": 0.3,
                        "max_tokens": 500
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data["choices"][0]["message"]["content"]
                else:
                    logger.error(f"DeepSeek error: {response.status_code}")
                    return None
                    
        except Exception as e:
            logger.error(f"DeepSeek connection error: {e}")
            return None


# === OpenAI Provider ===

class OpenAIProvider(LLMProvider):
    """
    Conector para OpenAI API.
    
    Usa GPT-4 Turbo para anÃ¡lisis mÃ¡s sofisticado.
    
    Pricing: ~$10 / 1M tokens (mÃ¡s caro pero potente)
    """
    
    BASE_URL = "https://api.openai.com/v1/chat/completions"
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or getattr(settings, 'openai_api_key', None)
        self.model = "gpt-4-turbo-preview"
    
    def is_available(self) -> bool:
        return bool(self.api_key)
    
    async def analyze(self, prompt: str) -> Optional[str]:
        if not self.is_available():
            return None
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.BASE_URL,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {
                                "role": "system",
                                "content": """You are a professional crypto trading analyst.
                                Analyze market data and provide clear signals.
                                Always respond in Spanish. Be concise.
                                Format: SIGNAL: BUY/SELL/HOLD, CONFIDENCE: %, REASON: ..."""
                            },
                            {
                                "role": "user",
                                "content": prompt
                            }
                        ],
                        "temperature": 0.3,
                        "max_tokens": 500
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data["choices"][0]["message"]["content"]
                else:
                    logger.error(f"OpenAI error: {response.status_code}")
                    return None
                    
        except Exception as e:
            logger.error(f"OpenAI connection error: {e}")
            return None


# === Ollama Provider (Local, Gratis) ===

class OllamaProvider(LLMProvider):
    """
    Conector para Ollama (ejecuciÃ³n local).
    
    Ejecuta modelos como Llama 3, Mistral, etc. en tu PC.
    
    ðŸ’° Gratis (corre en tu mÃ¡quina)
    âš¡ RÃ¡pido (sin latencia de red)
    ðŸ”’ Privado (datos no salen de tu PC)
    
    Requiere instalar Ollama: https://ollama.ai
    """
    
    BASE_URL = "http://localhost:11434/api/generate"
    
    def __init__(self, model: str = "llama3"):
        self.model = model
    
    def is_available(self) -> bool:
        try:
            import httpx
            response = httpx.get("http://localhost:11434/api/tags", timeout=2.0)
            return response.status_code == 200
        except:
            return False
    
    async def analyze(self, prompt: str) -> Optional[str]:
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    self.BASE_URL,
                    json={
                        "model": self.model,
                        "prompt": f"""Eres un analista de trading profesional.
                        Analiza estos datos y da una seÃ±al clara.
                        
                        {prompt}
                        
                        Responde con: SIGNAL: BUY/SELL/HOLD, CONFIDENCE: %, REASON: ...""",
                        "stream": False
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get("response", "")
                else:
                    return None
                    
        except Exception as e:
            logger.error(f"Ollama connection error: {e}")
            return None


# === LLM Manager ===

class LLMManager:
    """
    Gestor de LLMs con fallback automÃ¡tico.
    
    Orden de prioridad:
    1. DeepSeek (econÃ³mico y bueno)
    2. OpenAI (potente pero caro)
    3. Ollama (local y gratis)
    4. Sin LLM (usa solo ML local)
    """
    
    def __init__(self):
        self.providers: List[LLMProvider] = [
            DeepSeekProvider(),
            OpenAIProvider(),
            OllamaProvider()
        ]
        self._active_provider: Optional[LLMProvider] = None
        self._find_active_provider()
    
    def _find_active_provider(self):
        """Encontrar el primer proveedor disponible"""
        for provider in self.providers:
            if provider.is_available():
                self._active_provider = provider
                logger.info(f"ðŸ¤– LLM activo: {provider.__class__.__name__}")
                return
        
        logger.warning("âš ï¸ NingÃºn LLM disponible. Usando solo ML local.")
    
    @property
    def is_available(self) -> bool:
        return self._active_provider is not None
    
    @property
    def provider_name(self) -> str:
        if self._active_provider:
            return self._active_provider.__class__.__name__
        return "None"
    
    async def analyze_market(
        self,
        symbol: str,
        current_price: float,
        indicators: Dict,
        patterns: List[str],
        recent_signals: List[Dict]
    ) -> Optional[Dict]:
        """
        AnÃ¡lisis de mercado usando LLM.
        
        El LLM recibe contexto completo y proporciona:
        - SeÃ±al con razonamiento
        - Nivel de confianza
        - Factores de riesgo
        """
        if not self._active_provider:
            return None
        
        # Construir prompt con contexto
        prompt = f"""
        ANÃLISIS DE MERCADO: {symbol}
        
        ðŸ“Š Precio actual: ${current_price:,.2f}
        
        ðŸ“ˆ Indicadores tÃ©cnicos:
        - RSI: {indicators.get('rsi', 'N/A')}
        - MACD Histogram: {indicators.get('macd', 'N/A')}
        - Tendencia: {indicators.get('trend', 'N/A')}
        - ATR (volatilidad): {indicators.get('atr', 'N/A')}
        
        ðŸ” Patrones detectados: {', '.join(patterns) if patterns else 'Ninguno'}
        
        ðŸ“œ Ãšltimas seÃ±ales generadas:
        {self._format_recent_signals(recent_signals)}
        
        Proporciona tu anÃ¡lisis con:
        1. SIGNAL: BUY, SELL, o HOLD
        2. CONFIDENCE: porcentaje de confianza (0-100)
        3. REASON: explicaciÃ³n breve
        4. RISK: factores de riesgo a considerar
        5. TARGETS: niveles de entrada, stop-loss y take-profit sugeridos
        """
        
        try:
            response = await self._active_provider.analyze(prompt)
            
            if response:
                return self._parse_llm_response(response, symbol, current_price)
            
            return None
            
        except Exception as e:
            logger.error(f"Error en anÃ¡lisis LLM: {e}")
            return None
    
    def _format_recent_signals(self, signals: List[Dict]) -> str:
        if not signals:
            return "No hay seÃ±ales previas"
        
        lines = []
        for s in signals[-3:]:  # Ãšltimas 3
            lines.append(f"- {s.get('direction', '?')} @ ${s.get('price', 0):,.2f} ({s.get('result', 'pendiente')})")
        
        return "\n".join(lines)
    
    def _parse_llm_response(self, response: str, symbol: str, price: float) -> Dict:
        """Parsear respuesta del LLM"""
        result = {
            "symbol": symbol,
            "price": price,
            "signal": "HOLD",
            "confidence": 50,
            "reasoning": response,
            "source": self.provider_name,
            "timestamp": datetime.utcnow()
        }
        
        # Extraer seÃ±al
        response_upper = response.upper()
        if "SIGNAL: BUY" in response_upper or "SEÃ‘AL: COMPRA" in response_upper:
            result["signal"] = "BUY"
        elif "SIGNAL: SELL" in response_upper or "SEÃ‘AL: VENTA" in response_upper:
            result["signal"] = "SELL"
        
        # Extraer confianza
        import re
        confidence_match = re.search(r'CONFIDENCE:\s*(\d+)', response_upper)
        if confidence_match:
            result["confidence"] = min(int(confidence_match.group(1)), 100)
        
        return result


# === Singleton ===

_llm_manager: Optional[LLMManager] = None


def get_llm_manager() -> LLMManager:
    """Obtener gestor de LLMs"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager()
    return _llm_manager
